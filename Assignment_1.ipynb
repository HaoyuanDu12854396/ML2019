{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaoyuanDu12854396/ML2019/blob/master/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7LHdBqkuTMr",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 1: Report about [SHA48] INFORMATION THEORY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dfd9DkXyvP-T"
      },
      "source": [
        "##Introduction\n",
        "I choose the [SHA48] INFORMATION THEORY as my paper, because I want to know more things about information theory SHA48 was *A Mathematical Theory of Communication written* on *Ball System Technical Journal*, which is written by Claude Elwood Shannon in 1948. Shannon’s article laid the foundation for information theory, so he was called the \"father of information theory\".For half a century, information theory has been continuously improved, enriched and developed. Until today, shannon information theory has not only played a guiding role in the field of information engineering, but also found applications in other fields, such as statistical inference, computer science and the like.\n",
        "\n",
        "#Content\n",
        "In this report, he first proposed a method for achieving reliable communication in an interfered channel, and pointing out that the method for achieving efficient and reliable information transmission is coding.\n",
        "\n",
        "He also proposed a unit for measuring information. In his view, information, like physical properties of length and weight, is something that can be measured and regulated. For information systems, the information it conveys is random, so quantitative description information should be based on random events. He also mentioned that any information is redundant, and the size of the redundancy is related to the probability or uncertainty of the occurrence of each symbol (number, letter or word) in the message. Redundancy, the size of redundancy is related to the probability or uncertainty of occurrence of each symbol (number, letter or word) in the message.\n",
        "\n",
        "At the same time, Shannon proposed using information entropy to quantitatively measure the size of information. Let us first set the uncertainty of the occurrence of random events as the function f($p_i$) of the probability $p_i$. The function has the following three properties:\n",
        "\n",
        "1) Monotonicity: The event with higher probability, the smaller the information entropy, that is, $p_i$ and f($p_i$) are inversely proportional.\n",
        "\n",
        "2) Non-negative: f($p_i$) is non-negative;\n",
        "\n",
        "3) Additivity: A measure of the total uncertainty of simultaneous occurrence of multiple random events, which can be expressed as the sum of the various time uncertainty measures.\n",
        "\n",
        "Finally, Shannon proved from mathematics that the information entropy function satisfies the above properties and has the following unique form.\n",
        "\n",
        "\"The only H satisfying the three above assumptions is of the form:H = -K$\\sum_{i=1}^{n}{p_ilogp_i}$ \"（Shannon，p10）\n",
        "\n",
        "Information entropy not only quantitatively measures the size of information, but also provides the theoretical optimal value for information coding: the theoretical lower bound of the practical coded average code length is information entropy.\n",
        "\n",
        "In addition, Shannon has theoretically proved that as long as the communication rate is lower than the channel capacity C, a coding method can always be found, so that the error probability is close to zero. This conclusion shocked the entire communication theory community. The channel capacity C can be calculated simply by the Shannon formula, a simple and beautiful formula, based on the bandwidth and noise characteristics of the channel:\n",
        "\n",
        "\"If the noise is itself white, N = N1 and the result reduces to the formula proved previously:C = W log(1 + $\\frac{P}{N}$)\"（Shannon，p41）\n",
        "\n",
        "Where $\\frac{P}{N}$ is equal to the signal energy divided by the noise energy, ie the signal to noise ratio. And W represents the bandwidth of the channel.\n",
        "\n",
        "In this report, Shannon answer two basic questions:\n",
        "\n",
        "1. What is the value of critical data compression? (Information Entropy H)\n",
        "\n",
        "2. What is the value of the critical communication rate? (Channel Capacity C)\n",
        "\n",
        "\n",
        "#Innovation\n",
        "In this report, there are at least four points in the innovation.\n",
        "\n",
        "1. For information theory, this is a brand new field, an era-theory theory. \n",
        "\n",
        "2. For measuring information unit，before the report, people did not believe that information can be measured and standardized just like measuring height and weight. Only Shannon proposed this and proposed the bit unit.\n",
        "\n",
        "3. For Information entropy, this is a brand new noun for the field at the time. Shannon proposed using information entropy to quantitatively measure the size of the information. He expounds the information entropy, shows the relationship between information entropy and random event probability, and gives the formula that information entropy needs to be satisfied.\n",
        "\n",
        "4. For Shannon formula, before Shannon proposed information theory, it was generally believed that transmission systems that transmit information at a fixed rate without ignoring the probability of error are impossible. However, Shannon has theoretically proved that as long as the communication rate is lower than the channel capacity C, a coding method can always be found, so that the error probability is close to zero. This conclusion shocked the entire communication theory community.\n",
        "\n",
        "Shannon's article pioneered the great discipline of information theory, and he was therefore called the \"father of information theory\".\n",
        "#Technical Quality\n",
        "\n",
        "The technical development if of high quality.\n",
        "\n",
        "At the beginning of this article, Shannon describes the process of generating information through his general communication system diagram, and proposes three types of information systems (discrete, continuous and mixed) and one standard unit.\n",
        "Secondly, the concept of information entropy is proposed by analyzing DISCRETE NOISELESS SYSTEMS, and the relationship between information entropy and random event probability is derived step by step through pictures, formulas and examples.Then he decribe the situation in THE DISCRETE CHANNEL WITH NOISE and through the analysis of CONTINUOUS INFORMATION, the formula of signal power is gradually derived.\n",
        "\n",
        "Then, Shannon theoretically proved that as long as the communication rate is lower than the channel capacity C, an encoding method can always be found, so the error can be simply calculated by the Shannon formula, a simple and beautiful formula based on the bandwidth of the channel. And noise characteristics.\n",
        "\n",
        "Shannon puts it in fact that when we have a continuous source, we are not interested in the exact transmission, but are transmitted within a certain tolerance. The problem is that we can specify a certain rate for continuous sources when we only need a certain degree of recovery fidelity measured in an appropriate way. Of course, as the fidelity requirements increase, the rate will increase. Therefore, he defines such a rate in a very general case, by properly encoding the information, having the possible characteristics, transmitting it through a channel having a capacity equal to the rate in question, and satisfying the fidelity requirement, and proves this.\n",
        "\n",
        "Finally, an appendix shows some additional evidence.\n",
        "\n",
        "This article gives at least one theory, one unit, 23 theorems, two inter-temporal formulas, and countless arguments.\n",
        "#Application and X-Factor\n",
        "\n",
        "I find the proposal in the paper promising. This theory proposed by Shannon not only plays a guiding role in the field of communication engineering, but also in various fields such as probability theory, statistical inference, computer science, discrete mathematics, physics, economics, and chemistry.\n",
        "\n",
        "Some of them are more mature applications, like Active networking, Cyptography, Entropy in thermodynamics and information theory, Gambling, Seismic exploration and et. (wikipedia)\n",
        "\n",
        "I like this theory because it quantifies the information that cannot be touched. It has helped us a lot in all aspects, and it will be applied to explore science, the world and the universe in more ways.\n",
        "#Presentation\n",
        "The overall structure is clear. I find reading is not easy, because this article is very deep. If the author provides more explanations about the formula and a second explanation of the conclusion, then this article may be more attractive. In any case, this article has been very attractive. It shows the theory of trans-age, and its very rigorous academics elaborate the reasoning through various angles, and clearly explain the viewpoint.\n",
        "#References\n",
        "C. E. SHANNON 1948，’A Mathematical Theory of Communication‘， *Ball System Technical Journal* ,pp.379-423\n",
        "\n",
        "Wikipedia 2019, Information theory, <https://en.wikipedia.org/wiki/Information_theory#Concepts>"
      ]
    }
  ]
}